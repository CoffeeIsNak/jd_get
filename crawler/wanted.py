import requests
import time

def wanted_crawler(wanted_headers):
    API_URL = "https://www.wanted.co.kr/api/chaos/navigation/v1/results"
    WANTED_HEADERS = wanted_headers

    PARAMS = {
        "job_group_id": 518,  # ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì§êµ° ì½”ë“œ (ì˜ˆì‹œ)
        "job_ids": 655,  # íŠ¹ì • ì§ë¬´ ID (ì˜ˆì‹œ)
        "country": "kr",
        "job_sort": "job.latest_order",
        "years": [0, 3],  # 0~3ë…„ ê²½ë ¥
        "locations": "all",
        "limit": 20,  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜
        "offset": 0  # í˜ì´ì§€ë„¤ì´ì…˜ (ì²˜ìŒë¶€í„° ì‹œì‘)
    }

    all_jobs = []  # ëª¨ë“  ì±„ìš© ê³µê³  ì €ì¥ ë¦¬ìŠ¤íŠ¸
    offset = 0  # ì´ˆê¸° offset ê°’
    limit = 20  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜

    markdown_all = ""  # ì „ì²´ ê³µê³ 
    markdown_newbie = ""  # ì‹ ì… ê³µê³ 
    markdown_experienced = ""  # ê²½ë ¥ ê³µê³ 

    while True:
        PARAMS["offset"] = offset  # offset ì—…ë°ì´íŠ¸
        response = requests.get(API_URL, params=PARAMS, headers=WANTED_HEADERS)

        if response.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            break

        data = response.json()
        
        job_list = data.get("data", {})  # ì‹¤ì œ ì±„ìš© ê³µê³  ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        if not job_list:
            print("âœ… ë” ì´ìƒ ë°ì´í„° ì—†ìŒ â†’ í¬ë¡¤ë§ ì¢…ë£Œ")
            break  # ë” ì´ìƒ ë°ì´í„° ì—†ìœ¼ë©´ ì¢…ë£Œ

        all_jobs.extend(job_list)  # ê°€ì ¸ì˜¨ ë°ì´í„° ì¶”ê°€

        print(f"âœ… {offset} ~ {offset + limit}ë²ˆì§¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ ({len(job_list)}ê°œ)")
        
        for job in job_list:
            # ê³µê³  ì œëª©
            title = job.get("position", "ì œëª© ì—†ìŒ")

            # ì›í‹°ë“œ URL (id ê¸°ë°˜ ìƒì„±)
            job_id = job.get("id")
            job_url = f"https://www.wanted.co.kr/wd/{job_id}" if job_id else "URL ì—†ìŒ"

            # íšŒì‚¬ëª…
            company_name = job.get("company", {}).get("name", "íšŒì‚¬ëª… ì—†ìŒ")

            # ê·¼ë¬´ ì§€ì—­
            location = job.get("address", {}).get("district", "ì§€ì—­ ì •ë³´ ì—†ìŒ")

            # ê²½ë ¥ ì •ë³´
            is_newbie = job.get("is_newbie", False)  # ì‹ ì… ì—¬ë¶€
            annual_from = job.get("annual_from")  # ìµœì†Œ ê²½ë ¥
            annual_to = job.get("annual_to")  # ìµœëŒ€ ê²½ë ¥

            if is_newbie:
                career_info = "ì‹ ì…"
            elif annual_from is not None and annual_to is not None:
                career_info = f"{annual_from}~{annual_to}ë…„ ê²½ë ¥"
            else:
                career_info = "ê²½ë ¥ ì •ë³´ ì—†ìŒ"

            # ë§ˆê°ì¼ ì •ë³´ (D-ê°’ ì—†ìŒ)
            deadline = "ìƒì‹œì±„ìš©"

            # ë§ˆí¬ë‹¤ìš´ í¬ë§·ìœ¼ë¡œ ì €ì¥
            markdown_entry = f"\nğŸ”¹ Job: {title} ({company_name})\n"
            markdown_entry += f"ğŸ”— URL: {job_url}\n\n"
            markdown_entry += f"ğŸ“Œ **ìƒì„¸ ì •ë³´**: ['{career_info}', 'í•™ë ¥ë¬´ê´€', 'ì •ê·œì§', '{location}', '{deadline}']\n\n"
            markdown_entry += "---\n"

            # ì „ì²´ ë§ˆí¬ë‹¤ìš´
            markdown_all += markdown_entry

            # ì‹ ì… & ê²½ë ¥ êµ¬ë¶„ ì €ì¥
            if is_newbie:
                markdown_newbie += markdown_entry  # ì‹ ì… ì±„ìš© ê³µê³  ì €ì¥
            else:
                markdown_experienced += markdown_entry  # ê²½ë ¥ ì±„ìš© ê³µê³  ì €ì¥

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        offset += limit

        # ìš”ì²­ ì‚¬ì´ ë”œë ˆì´ ì¶”ê°€ (ì„œë²„ ì°¨ë‹¨ ë°©ì§€)
        time.sleep(1.5)


    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
    job_list_folder = "job_list"
    with open(job_list_folder + "/all_job_list.md", "w", encoding="utf-8") as f:
        f.write(markdown_all)

    with open(job_list_folder + "/newbie_job_list.md", "a", encoding="utf-8") as f:
        f.write(markdown_newbie)

    with open(job_list_folder + "/1_to_3_experience_required_job_list.md", "w", encoding="utf-8") as f:
        f.write(markdown_experienced)

    print(f"âœ… ì´ {len(all_jobs)}ê°œì˜ ì±„ìš© ê³µê³  ì €ì¥ ì™„ë£Œ! (wanted_all.md, wanted_newbie.md, wanted_experienced.md)")
